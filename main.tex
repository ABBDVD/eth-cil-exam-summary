\documentclass[a4paper, 11pt, landscape]{article}

\usepackage{mathptmx} % more compact font
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{multicol}
\usepackage{enumitem}
\usepackage{paralist} % for compacter lists
\usepackage{hyperref} % for Todo's and similar things
\usepackage[margin=0.5cm, landscape, nohead, nofoot]{geometry}
\usepackage[small,compact]{titlesec}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
\usepackage{xparse}

% compact text
\linespread{0.9}
\setlength{\parindent}{0pt}

% compact lists even more
\setdefaultleftmargin{0em}{0em}{0em}{0em}{0em}{0em}

% compact sections
\titlespacing*{\section}{0pt}{0em}{0em}
\titlespacing*{\subsection}{0pt}{0em}{0em}
\titlespacing*{\subsubsection}{0pt}{0em}{0em}

% coloured section headings for easier read
\titleformat{name=\section}[block]
  {\sffamily}
  {}
  {0pt}
  {\colorsection}
\newcommand{\colorsection}[1]{%
	\colorbox{red!10}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesection\ #1}}}


\titleformat{name=\subsection}[block]
  {\sffamily}
  {}
  {0pt}
  {\subcolorsection}
\newcommand{\subcolorsection}[1]{%
	\colorbox{orange!10}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesubsection\ #1}}}


\titleformat{name=\subsubsection}[block]
  {\sffamily}
  {}
  {0pt}
  {\subsubcolorsection}
\newcommand{\subsubcolorsection}[1]{%
	\colorbox{blue!10}{\parbox[t][0em]{\dimexpr\columnwidth-2\fboxsep}{\thesubsubsection\ #1}}}
	
% environment for multicols inside a list
\NewDocumentEnvironment{listcols}{O{2} O{0pt}}
	{%
		\bgroup %
		\setlength{\multicolsep}{#2} %
		\begin{multicols*}{#1} %
	}
	{%
		\end{multicols*} %
		\egroup %
	}

% multicols lines & spacing
\setlength{\columnsep}{0.2cm}
\setlength{\columnseprule}{0.2pt}

% No page numbers
\pagenumbering{gobble}

% math helpers
\newcommand{\matr}[1]{\boldsymbol{\mathrm{#1}}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}
\begin{multicols*}{3}

\section{PCA}
$\matr{X} \in \mathbb{R}^{D \times N}$. $N$ observations, $K$ properties. Target: $\tilde{\matr{X}} \in \mathbb{R}^{K \times N}$.
\begin{compactenum}
	\item Empirical Mean: $\overline{\matr{x}} = \frac{1}{N} \sum_{n=1}^N \matr{x}_n$
	\item Center Data: $\overline{\matr{X}} = \matr{X} - [\overline{\matr{x}}, \ldots, \overline{\matr{x}}] = \matr{X} - \matr{M}$
	\item Covariance Matrix: $\boldsymbol{\Sigma} = \frac{1}{N	} \sum_{n=1}^N (\matr{x}_n - \overline{\matr{x}}) (\matr{x}_n - \overline{\matr{x}})^T = \frac{1}{N} \overline{\matr{X}}\overline{\matr{X}}^T$
	\item Eigenvalue Decomposition: $\boldsymbol{\Sigma} = \matr{U \Lambda U}^T$, sort eigenvalues (and eigenvectors) in descending order
	\item Select $K < D$, keep only the first $K$ eigenvalues and corresponding eigenvectors $\Rightarrow \matr{U}_K, \matr{\Lambda}_K$
	\item Transform data onto new Basis: $\overline{\matr{Z}} = \matr{U}_K^T \overline{\matr{X}}$
	\item Reconstruct to original Basis: $\tilde{\overline{\matr{X}}} = \matr{U}_k \overline{\matr{Z}}$
	\item Reverse centring: $\tilde{\matr{X}} = \tilde{\overline{\matr{X}}} + \matr{M}$
\end{compactenum}

\begin{compactitem}
	\item For compression save $\matr{U}_k, \overline{\matr{Z}}, \overline{\matr{x}}$.
	\item $\matr{U}_k \in \mathbb{R}^{D \times K}, \boldsymbol{\Sigma} \in \mathbb{R}^{D \times D}, \overline{\matr{Z}} \in \mathbb{R}^{K \times N}, \overline{\matr{X}} \in \mathbb{R}^{D \times N}$
\end{compactitem}

\begin{compactdesc}
	\item[Approx. Error:] Sum of discarded eigenvalues
\end{compactdesc}

\begin{compactitem}
	\item TODO: Covariance Matrix of the result is diagonlized
	\item TODO: Properties needed to be known?
\end{compactitem}

\section{Eigenvalue / -vectors}
Eigenvalue Problem: $\matr{Ax} = \lambda \matr{x}$
\begin{compactenum}
	\item solve $\operatorname{det}(\matr{A} - \lambda \matr{I}) \overset{!}{=} 0$ resulting in $\{\lambda_i\}_i$
	\item $\forall \lambda_i$:
		solve $(\matr{A} - \lambda_i \matr{I}) \matr{x}_i = \matr{0}$, $\matr{x}_i$ is the $i$-th eigenvector.
	\item (opt.) normalize eigenvector $q_i$: $q_i^{\text{norm}} = \frac{1}{\|q_i\|_2} q_i$.
\end{compactenum}

\section{Eigendecomposition}
$\matr{A} \in \mathbb{R}^{N \times N}$ with $N$ linear independent eigenvectors, then: $\matr{A} = \matr{Q \Lambda Q}^{-1}$. $\matr{Q} \in \mathbb{R}^{N \times N}$, $i$-th column is the $i$-th eigenvector of $\matr{A}$. $\matr{\Lambda}$ is diagonal and $\matr{\Lambda}_{i,i}$ is the $i$-th eigenvector. Every symmetric matrix can be ``eigendecomposed''. 
$\matr{A}^{-1} = \matr{Q} \matr{\Lambda}^{-1} \matr{Q}^{-1}$, $\left[ \matr{\Lambda}^{-1} \right]_{i,i} = \frac{1}{\lambda_i}$ 
Calculate eigenvalues $\lambda_i$ and eigenvectors $q_i$. Normalise the eigenvectors. Put eigenvectors as columns into $\matr{Q}$ and eigenvalues into the diagonal of $\matr{\Lambda}$.

\section{SVD}
\begin{compactitem}
	\item $\matr{A} = \matr{U} \matr{D} \matr{V}^T = \sum_{k=1}^{\operatorname{rank}(\matr{A})} d_{k,k} u_k (v_k)^T$
	\item $\matr{A} \in \mathbb{R}^{N \times P}, \matr{U} \in \mathbb{R}^{N \times N}, \matr{D} \in \mathbb{R}^{N \times P}, \matr{V} \in \mathbb{R}^{P \times P}$
	\item $\matr{U}^T \matr{U} = I = \matr{V}^T \matr{V}$ ($\matr{U}, \matr{V}$ columns are orthonormal)
	\item $\matr{U}$ columns are eigenvectors of $\matr{A} \matr{A}^T$, $\matr{V}$ columns are eigenvectors of $\matr{A}^T \matr{A}$, $\matr{D}$ diagonal elements are singular values, i.e. the square roots of the eigenvalues ($\matr{A}^T \matr{A}$ and $\matr{A} \matr{A}^T$ have the same eigenvalues)
	\begin{listcols}
		\item $\matr{A}^T = \matr{V} \matr{D}^T \matr{U}^T$
		\item $\matr{A}^T \matr{A} = \matr{V} \matr{D}^T \matr{D} \matr{V}^T$
		\item $\matr{A} \matr{A}^T = \matr{U} \matr{D} \matr{D}^T \matr{U}^T$
	\end{listcols}
	\item $(\matr{D}^{-1})_{i,i} = \frac{1}{\matr{D}_{i, i}}$ ($\matr{D} \in \mathbb{R}^{N \times P} \to \matr{D}^{-1} \in \mathbb{R}^{P \times N}$, i.e. don't forget to transpose)
	\item Missing columns in $\matr{U}$ are basis of $\operatorname{null}(A^T)$ and in $\matr{V}$ are basis of $\operatorname{null}(A)$. Calculating: $\matr{A}^T \matr{u} = \matr{0}$ or $\matr{A} \matr{v} = \matr{0}$ for $\matr{u}$ or $\matr{v}$.
\end{compactitem}

\begin{compactenum}
	\item calculate $\matr{A}^T \matr{A}$.
	\item calculate eigenvalues of $\matr{A}^T \matr{A}$, the square root of them are the diagonal elements of $\matr{D}$.
	\item calculate eigenvectors of $\matr{A}^T \matr{A}$ using the eigenvalues resulting in the columns of $\matr{V}$.
	\item calculate the missing matrix: $\matr{U} = \matr{A} \matr{V} \matr{D}^{-1}$. Can be checked by calculating the eigenvectors of $\matr{A} \matr{A}^T$.
	\item normalize each columns of $\matr{U}$ and $\matr{V}$.
\end{compactenum}

\subsection{Low-Rank approximation}
Using only $K$ largest eigenvalues and corresponding eigenvectors. $\tilde{\matr{A}}_{i, j} = \sum_{k}^K \matr{U}_{i, k} \matr{D}_{k,k} \matr{V}_{j, k}$.
\begin{compactdesc}
	\item[Approx. Error Frobenius Norm:] $\|\matr{A} - \tilde{\matr{A}}\|_F = \sqrt{\sum_{i > K} \sigma_i^2} = \sqrt{\sum_{i > K} \lambda_i}$
	\item[Approx. Error Euclidean Norm:] $\|\matr{A} - \tilde{\matr{A}}\|_2 = \sigma_{K+1}$ (i.e. largest singular value that was thrown away)
\end{compactdesc}

\section{Basis Transform}
$\matr{V}, \matr{W}$ are two basis matrices (each column is a basis vector, invertible). Representing $\matr{x}$ in basis $\matr{V}$ by \emph{coordinate vector} $\matr{a}$: $\matr{x} = \matr{V} \matr{a} \Rightarrow \matr{a} = \matr{V}^{-1} \matr{x}$. Knowing $\matr{a}$ representing $\matr{x}$ in $\matr{V}$ we can calculate coordinate vector $\matr{b}$ representing $\matr{x}$ in $\matr{W}$ with: $\matr{x} = \matr{W}^{-1} \matr{b} = \matr{V}^{-1} \matr{a} \Rightarrow \matr{b} = \matr{W} \matr{V}^{-1} \matr{a}$.

\section{Matrix/Vector}
\begin{compactdesc}
	\item[Symmetric:] $\matr{A} = \matr{A}^T$
	\item[Orthogonal:] (i.e. columns are orthonormal!) $\matr{A}^{-1} = \matr{A}^T$, $\matr{A} \matr{A}^T = \matr{A}^T \matr{A} = \matr{I}$, $\operatorname{det}(\matr{A}) \in \{+1, -1\}$, $\operatorname{det}(\matr{A}^T \matr{A}) = 1$
	\item[Inner Product:] (assuming everything in $\mathbb{R}^D$) $\langle \matr{x}, \matr{y} \rangle = \matr{x}^T \matr{y} = \sum_{i=1}^{N} \matr{x}_i \matr{y}_i$.
	\begin{compactitem}
		\item $\langle \matr{x} \pm \matr{y}, \matr{x} \pm \matr{y} \rangle = \langle \matr{x}, \matr{x} \rangle \pm 2 \langle \matr{x}, \matr{y} \rangle + \langle \matr{y}, \matr{y} \rangle$
		\begin{listcols}
			\item $\langle \matr{x}, \matr{y} + \matr{z} \rangle = \langle \matr{x}, \matr{y} \rangle + \langle \matr{x}, \matr{z} \rangle$
			\item $\langle \matr{x} + \matr{y}, \matr{z} \rangle = \langle \matr{x}, \matr{z} \rangle + \langle \matr{y}, \matr{z} \rangle$
			\item $\langle \matr{x}, \matr{y} \rangle = |\matr{x}| \cdot |\matr{y}| \cdot \cos(\theta)$
		\end{listcols}
		\item If $\matr{y}$ is a unit vector then $\langle \matr{x}, \matr{y} \rangle$ projects $\matr{x}$ onto $\matr{y}$
	\end{compactitem}
	\item[Outer Product:] $\matr{u} \matr{v}^T$, $(\matr{u} \matr{v}^T)_{i, j} = \matr{u}_i \matr{v}_j$
	\item[Transpose:]\hfill
	\begin{listcols}
		\begin{compactitem}
			\item $(\matr{A} + \matr{B})^T = \matr{A}^T + \matr{B}^T$
			\item $(\matr{A}\matr{B})^T = \matr{B}^T \matr{A}^T$
			\item $(\alpha \matr{A})^T = \alpha \matr{A}^T$
			\item $(\matr{A}^T)^{-1} = (\matr{A}^{-1})^T$
		\end{compactitem}
	\end{listcols}
	\item[Gram-Schmidt:] $\{\matr{w}_i\}_i$ non-orthogonal basis. $\matr{v}_n = \matr{w}_n - \sum_{i=1}^{n-1} \frac{\langle \matr{v}_i, \matr{w}_n \rangle}{\langle \matr{v}_i, \matr{v}_i \rangle} \matr{v}_i$ results in $\{\matr{v}_i\}_i$ an orthogonal basis (same space).
\end{compactdesc}

\section{Norms}
\begin{compactdesc}
	\item[Euclidean:] $\|\matr{x}\|_2 := \sqrt{\sum_{i=1}^{N} \matr{x}_i^2} = \sqrt{\matr{x}^T \matr{x}} = \sqrt{\langle \matr{x}, \matr{x} \rangle}$
	\item[$p$-norm:] $\|\matr{x}\|_p := \left( \sum_{i=1}^{N} |x_i|^p \right)^{\frac{1}{p}}$
	\item[Frobenius:] $\|\matr{A}\|_F :=\allowbreak \sqrt{\sum_{i=1}^{M} \sum_{j=1}^{N} |\matr{A}_{i, j}|^2} =\allowbreak \sqrt{\operatorname{trace}(\matr{A}^T \matr{A})} =\allowbreak \sqrt{\sum_{i=1}^{\min\{m, n\}} \sigma_i^2}$ ($\sigma_i$ is the $i$-th singularvalue), $\matr{A} \in \mathbb{R}^{M \times N}$
	\item[Nuclear:] $\|\matr{X}\|_\star = \sum_{i=1}^{\min(m, n)} \sigma_i$
\end{compactdesc}

\section{$K$-means Algorithm}
\begin{compactenum}
	\item Initiate: choose randomly $K$ centroids $\matr{U} = [\matr{u}_1, \ldots, \matr{u}_K]$ (usually $K$ randomly chosen data points).
	\item Assign data points to clusters. $k^\star(\matr{x}_n) = \argmin_k \{ \|\matr{x}_n - \matr{u}_1\|_2^2, \ldots, \|\matr{x}_n - \matr{u}_K\|_2^2 \}$ gives us the cluster $k$ whose centroid $\matr{u}_k$ is closest to data point $\matr{x}_n$. Set accordingly the $k$-th element of the $n$-th column of the assignment matrix $\matr{Z}$ to $1$, others to $0$.
	\item Update centroids: $\matr{u}_k = \frac{\sum_{n=1}^N z_{k,n} \matr{x}_n}{\sum_{n=1}^N z_{k,n}}$ (each centroid is in the middle of all data points assigned to it's cluster).
	\item Repeat from step 2.
\end{compactenum}
Iteration stops if the half of the number of changes in $\matr{Z}$ is zero ($\frac{1}{2} \|\matr{Z} - \matr{Z}^\text{new}\|_0$).

\section{Probability / Statistics}
\begin{compactitem}
	\item $P(x) := Pr[X = x] := \sum_{y \in Y} P(x, y)$
	\item $P(x|y) := Pr[X = x | Y = y] := \frac{P(x,y)}{P(y)},\quad \text{if } P(y) > 0$
	\item $\forall y \in Y: \sum_{x \in X} P(x|y) = 1$ (property for any fixed $y$)
	\item $P(x, y) = P(x|y) P(y)$
	\item $\sum_{y \in Y} P(x, y) = P(x)$
	\item $P(x|y) = \frac{P(y|x)P(x)}{P(y)}$ (Bayes' rule)
	\item Two random variables $X$, $Y$ are called \emph{independent}, if knowing one of them \textbf{doesn't} reveal any information about the other one: $P(x|y) = P(x) \Leftrightarrow P(y|x) = P(y)$
	\item \emph{IID} (Independent and Identically Distributed): $n$ random variables are IID if each has the same distribution and they are mutually independent.
	\begin{compactitem}
		\item $P(x_1, \ldots, x_n) = \prod_{i=1}^n P(x_i)$
	\end{compactitem}
\end{compactitem}

\section{Gaussian Mixture Models (GMM)}
\begin{compactdesc}
	\item[GMM:] $p(\matr{x}) = \sum_{k=1}^K \pi_k \mathcal{N}(\matr{x} | \boldsymbol{\mu}_k, \Sigma_k)$
	\item[Assignment variable:] $\matr{z}_k \in \{0, 1\}$, $\sum_{k=1}^K \matr{z}_k = 1$, $\operatorname{Pr}(\matr{z}_k = 1) = \boldsymbol{\pi}_k \Leftrightarrow p(\matr{z}) = \prod_{k=1}^K \boldsymbol{\pi}_k^{\matr{z}_k}$
	\item[Complete data distribution:] $p(\matr{x}, \matr{z}) = \prod_{k=1}^K \left( \boldsymbol{\pi}_k \mathcal{N}(\boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)^{\matr{z}_k}$
	\item[Posterior Probabilities:] $\operatorname{Pr}(\matr{z}_k = 1 | \matr{x}) = \frac{\operatorname{Pr}(\matr{z}_k = 1) p(\matr{x} | \matr{z}_k = 1)}{\sum_{l=1}^K \operatorname{Pr}(\matr{z}_l = 1) p(\matr{x} | \matr{z}_l = 1)} = \frac{\boldsymbol{\pi}_k \mathcal{N}(\matr{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)}{\sum_{l=1}^K \boldsymbol{\pi}_l \mathcal{N}(\matr{x} | \boldsymbol{\mu}_l, \boldsymbol{\Sigma}_l)}$
	\item[Likelihood of observed data $\matr{X}$:] $p(\matr{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) = \prod_{n=1}^N p(\matr{x}_n) = \prod_{n=1}^N \sum_{k=1}^N \pi_k \mathcal{N}(\matr{x} | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k)$
	\item[Log-likelihood:] $\log p(\matr{X} | \boldsymbol{\pi}, \boldsymbol{\mu}, \boldsymbol{\Sigma}) =\break \sum_{i=1}^N \log \left( \sum_{k=1}^K \pi_k \mathcal{N}(\matr{x}_i | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)$
	\item[Negative Log-likelihood:] The smaller, the better the fit: $-\ln(p(\matr{X} | \matr{\theta})) = -\Sigma_{n=1}^N \ln\left( \Sigma_{k=1}^K \boldsymbol{\pi}_k \mathcal{N}(\matr{x}_n | \boldsymbol{\mu}_k, \boldsymbol{\Sigma}_k) \right)$
\end{compactdesc}
\begin{compactitem}
	\item Generative: given $\matr{z}$, generate $\matr{x}$
	\item Inference: given $\matr{x}$, infer $\matr{z}$
\end{compactitem}


\subsection{AIC / BIC for GMM}
Trade-off: balance between data fit (i.e. likelihood $p(\matr{X} | \theta)$) and complexity (i.e. \# of free parameters $\kappa(\cdot)$). Heuristics:
\begin{compactdesc}
	\item[Akaike Information Criterion (AIC):] $\operatorname{AIC}(\theta | \matr{X}) = -\ln p_\theta(\matr{X}) + \kappa(\theta)$
	\item[Bayesian Information Criterion (BIC):] $-\ln(p_\theta(\matr{x})) + \frac{1}{2} \kappa(\theta) \ln(N)$
	\item[\# of free params:] $\kappa(\theta) = K \cdot D + T$ ($K$ number of clusters, $D$ number of $\mu$'s, $T$ number of not fixed $\pi$'s (usually $T=(K-1)$ as the last $\pi$ is defined by the others (sum up to $1$)))
\end{compactdesc}
Note that the first term is the log-likelihood multiplied by $-1$. Compare AIC/BIC for different values of $K$, best GMM is the one with smallest AIC/BIC.


\section{Expectation-Maximization (EM)}
\begin{compactitem}
	\item Expectation step: Assign data to parts of the model (e.g. assign data point to a mixture component)
	\item Maximization step: Improve the model parameters based on the new assignment
	\item Iterative process, i.e. repeat until converges
	\item may not result in global maximum/optimum
\end{compactitem}

\subsection{EM for GMM}
\begin{compactenum}
	\item Initialize $\boldsymbol{\pi}_k^{(0)}, \boldsymbol{\mu}_k^{(0)}, \boldsymbol{\Sigma}_k^{(0)}, t = 1$ for $k = 1, \ldots, K$
	\item E-Step: $q_{k,n} := \mathbb{E}[z_{k,n}] = \frac{\boldsymbol{\pi}_k^{(t-1)} \mathcal{N}(\matr{x}_n | \boldsymbol{\mu}_k^{(t-1)}, \boldsymbol{\Sigma}_k^{(t-1)})}{\sum_{j=1}^K \boldsymbol{\pi}_j^{(t-1)} \mathcal{N}(\matr{x}_n | \boldsymbol{\mu}_j^{(t-1)}, \boldsymbol{\Sigma}_j^{(t-1)})}$
	\item M-Step: $\boldsymbol{\mu}_k^{(t)} := \frac{\sum_{n=1}^N q_{k,n} \matr{x}_n}{\sum_{n=1}^N q_{k,n}}$, $\boldsymbol{\pi}_k^{(t)} := \frac{1}{N} \sum_{n=1}^N q_{k,n}$, and $\Sigma_k^{(t)} = \frac{\sum_{n=1}^N q_{k, n} (\matr{x}_n - \boldsymbol{\mu}_k^{(t)})(\matr{x}_k - \boldsymbol{\mu}_k^{(t)})^T}{\sum_{n=1}^N q_{k,n}}$
	\item Repeat from (2.) with $t = t + 1$ if not $\| \log p(\matr{X} | \boldsymbol{\pi}^{(t)}, \boldsymbol{\mu}^{(t)}, \boldsymbol{\Sigma}^{(t)}) - \log p(\matr{X} | \boldsymbol{\pi}^{(t-1)}, \boldsymbol{\mu}^{(t-1)}, \boldsymbol{\Sigma}^{(t-1)}) \| < \epsilon$
\end{compactenum}


\subsection{EM for Inpainting}
TODO

\section{Haar Wavelet}
\begin{compactdesc}
	\item[Mother function:] $\psi(t)$ is $1$ for $0 \leq t < \frac{1}{2}$, $-1$ for $\frac{1}{2} \leq t < 1$, $0$ otherwise.
	\item[Haar function:] $\psi_{n,k}(t) = 2^{n/2} \psi(2^n t - k)$, $2^{n/2}$ only needed if normalized values needed!
\end{compactdesc}

\begin{compactitem}
	\item For the not normalized basis: $\matr{W}^{-1} = (\matr{W^T} \matr{W})^{-1} \matr{W}^T = \operatorname{diag}(\frac{1}{l_1}, \ldots, \frac{1}{l_n}) \matr{W}^T$ where $l_i$ is the norm of the $i$-th column of $\matr{W}$. This is valid because $\matr{W}$ is orthogonal!
	\item Don't forget that $[1, 1, \ldots, 1]^T$ is the first non-normalized Haar Wavelet basis!
	\item \textbf{``Coding'' step:} Tranform $\matr{x}$ into coordinate vector $\matr{c}$ for Haar Wavelets
	\item \textbf{``Thresholding'' step:} Set small absolute values in $\matr{c}$ to $0$ or remove them alongside the basis vector.
\end{compactitem}

\section{Optimization}
Gradient of $f: \mathbb{R}^D \to \mathbb{R}$ is $\nabla f(\matr{x}) := \left( \frac{\partial f(\matr{x})}{\partial \matr{x}_1}, \ldots, \frac{\partial f(\matr{x})}{\partial \matr{x}_D} \right)^T \in \mathbb{R}^D$

\subsection{Convex Functions}
A function $f(x)$ is convex if domain of $f$ (set of input values) is convex (e.g. $R^D$) and $\forall y, z, \forall t \in [0, 1]: f(t \cdot y + (1-t) \cdot z) \leq t \cdot f(y) + (1-t) \cdot f(z)$.

\subsection{Coordinate Descent}
Update one coordinate (the $d$-th) at a time, others are fixed.
\begin{compactenum}
	\item Initialize: $\matr{x}^{(0)} \in \mathbb{R}^D$
	\item For $t = 0 \ \text{to} \ \mathit{maxIters}$:
	\item choose $d$ uniformly from $1, \ldots, D$
	\item $u^\star = \argmin_{u \in \mathbb{R}} f(x_1^{(t)}, \ldots, u, \ldots, x_D^{(t)})$
	\item $\matr{x}_d^{(t+1)} = u^\star$ and $\matr{x}_i^{(t+1)} = \matr{x}_i^{(t)}$ for $i \neq d$
\end{compactenum}

\subsection{Gradient Descent}
\begin{compactenum}
	\item Initialize: $\matr{x}^{(0)} \in \mathbb{R}^D$
	\item For $t = 0 \ \text{to} \ \mathit{maxIters}$:
	\item $\matr{x}^{(t+1)} = \matr{x}^{(t)} - \gamma \nabla f(\matr{x}^{(t)})$ ($\gamma \approx \frac{1}{t}$ is the stepsize)
\end{compactenum}

\subsection{Stochastic Gradient Descent}
\begin{compactenum}
	\item Initialize: $\matr{x}^{(0)} \in \mathbb{R}^D$
	\item For $t = 0 \ \text{to} \ \mathit{maxIters}$:
	\item choose $n$ uniformly from $1, \ldots, D$
	\item $\matr{x}^{(t+1)} = \matr{x}^{(t)} - \gamma \nabla f_n(\matr{x}^{(t)})$ ($\gamma \approx \frac{1}{t}$ is the stepsize)
\end{compactenum}

\subsection{Projected Gradient Descent}
In the gradient update step use a projection into constraint space $Q \subseteq \mathbb{R}^D$ with $P_Q(\matr{x}) = \argmin_{y \in Q} \|y - x\|$: $\matr{x}^{(t+1)} = P_Q\left(\matr{x}^{(t)} - \gamma \nabla f(\matr{x}^{(t)})\right)$

\subsection{Lagrangian Multipliers}
Minimize  $f(\matr{x})$ subject to $g_i(\matr{x}) \leq 0,\ i = 1, \ldots, m$ and $h_i(\matr{x}) = \matr{a}_i^T \matr{x} - b_i = 0,\ i = 1, \ldots, p$.
\begin{compactdesc}
	\item[Lagrangian:] $L(\matr{x}, \matr{\lambda}, \matr{\nu}) := f(\matr{x}) + \sum_{i=1}^m \lambda_i g_i(\matr{x}) + \sum_{i=1}^p \nu_i h_i(\matr{x})$
	\item[Dual function:] $d(\matr{\lambda}, \matr{\nu}) := \inf_{\matr{x}} L(\matr{x}, \matr{\lambda}, \matr{\nu}) \in \mathbb{R}$
	\item[Dual Problem:] Maximize $d(\matr{\lambda}, \matr{\nu})$ with $\boldsymbol{\lambda} \geq \matr{0}$. Result is lower bound of optimum $f(\matr{x}^\star)$. Also only interested in $d(\cdot)$ where it is finite.
\end{compactdesc}

\section{Matching Pursuit (MP)}
\subsection{Over-Complete Dictionaries}
Sparse approximation of a vector $\matr{x}$ onto an over-complete dictionary $\mathcal{D}$ represented by matrix $\matr{U}$ using $K$ dictionary entries.
\begin{compactenum}
	\item Initialize: Coefficient vector $\mathbb{R}^{|\mathcal{D}|} \ni \matr{z} = \matr{0}$, Residual $\matr{r} = \matr{x}$
	\item While $\|\matr{z}\|_0 < K$ repeat the following steps
	\item Find dictionary entry with largest inner product between latest residual and a dictionary entry: $d^\star = \argmax_d |\langle \matr{u}_d, \matr{r} \rangle|$
	\item Update coefficients: $\matr{z}_{d^\star} = \matr{z}_{d^\star} + \langle \matr{u}_{d^\star}, \matr{r} \rangle$
	\item Update residual: $\matr{r} = \matr{r} - \langle \matr{u}_{d^\star}, \matr{r} \rangle \matr{u}_{d^\star}$.
\end{compactenum}

\subsection{Inpainting}
Diagonal matrix $\matr{M}$ with $\matr{M}_{i, i} = 1$ if $i$-th pixel known, otherwise $\matr{M}_{i,i} = 0$. Interested in $\matr{z}^\star \in \argmax_{\matr{z}} \|\matr{z}\|_0$ subject to $\|\matr{M}(\matr{x} - \matr{Uz})\|_2 < \sigma$. $\matr{U}$ over-complete dictionary of known parts, $\sigma$ allowed error. Reconstructed image: $\hat{\matr{x}} = \matr{Mx} + (\matr{I} - \matr{M})\matr{U} \matr{z}^\star$.
\begin{compactenum}
	\item Initialize: Coefficient vector $\matr{z} = \matr{0}$, residual $\matr{r} = \matr{M}\matr{x}$.
	\item While $\|z\|_0 < K$ and $\|r\|_2 > 0$ repeat the following steps
	\begin{listcols}[2][5pt]
		\item $d^\star = \argmax_d \frac{|\langle \matr{M}\matr{u}_d, \matr{r}\rangle|}{\|\matr{M}\matr{u}_d\|_2}$
		\item $\matr{z}_{d^\star} = \matr{z}_{d^\star} + \frac{\langle \matr{M}\matr{u}_{d^\star}, \matr{r} \rangle}{\|\matr{M}\matr{u}_{d^\star}\|_2^2}$
		\item $\matr{r} = \matr{r} - \frac{\langle \matr{M}\matr{u}_{d^\star}, \matr{r} \rangle}{\|\matr{M}\matr{u}_{d^\star}\|_2^2} \matr{M}\matr{u}_{d^\star}$
	\end{listcols}
\end{compactenum}


\section{Coherence ($m(\cdot)$)}
\begin{compactitem}
	\item $D$ dimension of data, $L$ number of atoms, $L > D$ then over-complete dictionary
	\item $m(\matr{U}) = \max_{i,j:\, i \neq j} | \matr{u}_i^T \matr{u}_j |$
	\item $m(\matr{B}) = 0$ if $\matr{B}$ is orthogonal matrix and a basis
	\item $m([\matr{B}, \matr{u}]) \geq \frac{1}{\sqrt{D}}$ if atom $\matr{u}$ is added to orthogonal $\matr{B}$
\end{compactitem}

\section{Robust PCA}
\begin{compactitem}
	\item Idea: Approximate $\matr{X}$ with $\matr{L} + \matr{S}$ where $\matr{L}$ is low-rank, $\matr{S}$ is sparse.
	\item Minimize $\|\matr{L}\|_\star + \lambda \|\matr{S}\|_1$ s. t. $\matr{L} + \matr{S} = \matr{X}$
	\item A perfect reconstruction is possible if $\matr{S}$ is not low-rank, $\matr{L}$ is not sparse, $\matr{X}$ is not low-rank \textit{and} sparse at the same time
\end{compactitem}

\section{TODO}
Check if everywehere orthogonal matrix == orthonormal columns!



\raggedcolumns
\end{multicols*}
\end{document}
